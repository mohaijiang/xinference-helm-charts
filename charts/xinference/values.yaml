# common used configurations
config:
  xinference_image: ""
  curl_image: curlimages/curl:8.8.0
  image_pull_policy: ""
  worker_num: 1
  gpu_per_worker: ""
  model_src: "huggingface"
  persistence:
    enabled: false
    mountPath: "/data"
  # any other environments
  extra_envs: {}

pvc:
  accessModes:
    # the volume can be mounted as read-write by many nodes.
    - ReadWriteMany
  sharedVolumeClaim:
    storageRequest: 10Gi
  storageClassName: ""
  volumeMode: ""
serviceWeb:
  nodePort: ""
  # change this to `LoadBalancer` if you use cloud platform
  type: NodePort
serviceSupervisor:
  ports:
  - name: service-supervisor-oscar
    port: 9999
    protocol: TCP
    targetPort: 9999
  - name: service-supervisor-web
    port: 9997
    protocol: TCP
    targetPort: 9997
  type: ClusterIP
serviceWorker:
  ports:
  - port: 30001
    protocol: TCP
    targetPort: 30001
  type: ClusterIP
xinferenceSupervisor:
  affinity: {}
  supervisor:
    args:
    - --port
    - "9997"
    - --host
    - $(POD_IP)
    - --supervisor-port
    - "9999"
    - --log-level
    - debug
    ports:
      - containerPort: 9997
        name: web
      - containerPort: 9999
        name: oscar
    resources:
      requests:
        cpu: "1"
        memory: 4Gi
xinferenceWorker:
  strategy:
    type: Recreate
  affinity: {}
  worker:
    initContainers:
      command: [ 'sh', '-c', "until curl -v http://service-supervisor:9997/v1/address; do echo waiting for supervisor; sleep 1; done" ]
    args:
    - -e
    - http://service-supervisor:9997
    - --host
    - $(POD_IP)
    - --worker-port
    - "30001"
    - --log-level
    - debug
    ports:
      - containerPort: 30001
    resources:
      requests:
        cpu: "2"
        memory: 8Gi
